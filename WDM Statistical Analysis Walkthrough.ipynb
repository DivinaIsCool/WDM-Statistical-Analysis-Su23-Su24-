{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f7764b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This jupyter notebook is a walkthrough of all methods used to compute the unbinned likelihood values of WDM models as a function of their thermal relic mass, among other parameters. This notebook will not go over any of the underlying physics or why we use the methods that we use. To understand those things, check out [this summary of methods and conclusions](https://github.com/DivinaIsCool/WDM-Statistical-Analysis-Su23-Su24-/blob/main/WDM_Paper%20GH.pdf) To start, we will need many different functions from numpy, scipy, matplotlib, vegas (our numerical integration method), disSat (useful module developed by Dr. Stacy Kim to compute a lot of the most common things required for dark matter research), and colossus (another module useful for cosmology and dark matter related tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc0a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT BLOCK\n",
    "\n",
    "import numpy as np\n",
    "import scipy.integrate as integ\n",
    "import scipy.special as spec\n",
    "import scipy.optimize as optim\n",
    "from scipy.ndimage import map_coordinates\n",
    "import scipy.optimize as optim\n",
    "import vegas\n",
    "import matplotlib.pyplot as plt\n",
    "import disSat as dis\n",
    "import disSat.dark_matter.models.wdm as wdm\n",
    "from colossus.cosmology import cosmology\n",
    "import colossus.halo as halo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4f546",
   "metadata": {},
   "source": [
    "# Preliminary Variables, Data, and Misc Things\n",
    "\n",
    "There are a few things we need to define now so that we can use them later on. First, we have the Cartesian grid interpolator, an interpolator that essentially does the job of np.interpolate much better. We will use this to interpolate from data values defined in a data file that I created that gives the WDM concentration as a function of halo mass and thermal relic mass. I decided to interpolate this information from a data file instead of just computing it here because it significantly reduced the time to compute. Next, we import out data of Milky Way satellite galaxies and assign arrays of line of sight stellar velocity dispersions and 2D half-light radii and associated errors. For the sake of this notebook running quickly, I have commented out most of these galaxies, but to get the actual likelihood they should all be uncommented. Then, we define many of the constants that will be used later on. These include the factor used to switch between different definitions of a lognormal distribution used by different modules, the cosmology (planck 18) from colossus, the infall time at z=1, various scatter values, and other cosmology  parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4258a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cartesian grid interpolator\n",
    "class CartesianGridInterpolator:\n",
    "    \"\"\"\n",
    "    This class works as scipy.interpolate.RegularGridInterpolator, but\n",
    "    it's optimized for equally-spaced grids.\n",
    "\n",
    "    Obtained from\n",
    "    https://docs.scipy.org/doc/scipy/tutorial/interpolate/ND_regular_grid.html\n",
    "    \"\"\"\n",
    "    def __init__(self, points, values, method='linear'):\n",
    "        self.limits = np.array([[min(x), max(x)] for x in points])\n",
    "        self.values = np.asarray(values, dtype=float)\n",
    "        self.order = {'linear': 1, 'cubic': 3, 'quintic': 5}[method]\n",
    "\n",
    "    def __call__(self, xi):\n",
    "        \"\"\"\n",
    "        `xi` here is an array-like (an array or a list) of points.\n",
    "\n",
    "        Each \"point\" is an ndim-dimensional array_like, representing\n",
    "        the coordinates of a point in ndim-dimensional space.\n",
    "        \"\"\"\n",
    "        # transpose the xi array into the ``map_coordinates`` convention\n",
    "        # which takes coordinates of a point along columns of a 2D array.\n",
    "        xi = np.asarray(xi).T\n",
    "\n",
    "        # convert from data coordinates to pixel coordinates\n",
    "        ns = self.values.shape\n",
    "        coords = [(n-1)*(val - lo) / (hi - lo)\n",
    "                  for val, n, (lo, hi) in zip(xi, ns, self.limits)]\n",
    "\n",
    "        # interpolate\n",
    "        return map_coordinates(self.values, coords,\n",
    "                               order=self.order,\n",
    "                               cval=np.nan)  # fill_value\n",
    "    \n",
    "# Subhalo data block\n",
    "dwarfs_updated = {#'.Boötes I': {'reff': 191.0,'refferr': [5.0, 5.0],'sigma': 5.1,'sigerr': [0.8, 0.7]},\n",
    "                  # '.Boötes II': {'reff': 38.7,'refferr': [5.1, 5.1],'sigma': 2.9,'sigerr': [1.2, 1.6]},\n",
    "                  #'.Canes Venatici I': {'reff': 452.0,'refferr': [13.0, 13.0],'sigma': 7.6,'sigerr': [0.4, 0.4]},\n",
    "                  #'.Canes Venatici II': {'reff': 70.7,'refferr': [11.2, 11.2],'sigma': 4.6,'sigerr': [1.0, 1.0]},\n",
    "                  #'.Carina': {'reff': 308.0,'refferr': [3.0, 3.0],'sigma': 6.6,'sigerr': [1.2, 1.2]},\n",
    "                  #'.Coma Berenices': {'reff': 72.1,'refferr': [3.8, 3.8],'sigma': 4.6,'sigerr': [0.8, 0.8]},\n",
    "                  #'.Draco': {'reff': 214.0,'refferr': [2.0, 2.0],'sigma': 9.1,'sigerr': [1.2, 1.2]},\n",
    "                  #'.Fornax': {'reff': 838.0,'refferr': [3.0, 3.0],'sigma': 11.7,'sigerr': [0.9, 0.9]},\n",
    "                  #'.Hercules': {'reff': 216.0,'refferr': [17.0, 17.0],'sigma': 5.1,'sigerr': [0.9, 0.9]},\n",
    "                  #'.Leo I': {'reff': 270.0,'refferr': [2, 2],'sigma': 9.2,'sigerr': [0.4, 0.4]},\n",
    "                  #'.Leo II': {'reff': 171.0,'refferr': [2.0, 2.0],'sigma': 7.4,'sigerr': [0.4, 0.4]},\n",
    "                  #'.Leo IV': {'reff': 114.0,'refferr': [13.0, 13.0],'sigma': 3.4,'sigerr': [0.9, 1.3]},\n",
    "                  #'.Leo V': {'reff': 49.0,'refferr': [16.0, 16.0],'sigma': 2.3,'sigerr': [1.6, 3.2]},\n",
    "                  #'.Sculptor': {'reff': 280.0,'refferr': [1.0, 1.0],'sigma': 9.2,'sigerr': [1.1, 1.1]},\n",
    "                  #'.Segue 1': {'reff': 24.2,'refferr': [2.8, 2.8],'sigma': 3.7,'sigerr': [1.1, 1.4]},\n",
    "                  #'.Sextans': {'reff': 413.0,'refferr': [3.0, 3.0],'sigma': 7.9,'sigerr': [1.3, 1.3]},\n",
    "                  #'.Ursa Major I': {'reff': 234.0,'refferr': [10.0, 10.0],'sigma': 7.0,'sigerr': [1.0, 1.0]},\n",
    "                  '.Ursa Major II': {'reff': 128.0,'refferr': [5.0, 5.0],'sigma': 5.6,'sigerr': [1.4, 1.4]},\n",
    "                  '.Ursa Minor': {'reff': 405.0,'refferr': [21.0, 21.0],'sigma': 9.5,'sigerr': [1.2, 1.2]},\n",
    "                  '.Willman 1': {'reff': 27.7,'refferr': [2.4, 2.4],'sigma': 4.0,'sigerr': [0.8, 0.8]}\n",
    "                 }\n",
    "\n",
    "sigmas = []\n",
    "reffs = []\n",
    "sigerr_upper = []\n",
    "sigerr_lower = []\n",
    "refferr_upper = []\n",
    "refferr_lower = []\n",
    "names = []\n",
    "\n",
    "for dwarf, properties in dwarfs_updated.items():\n",
    "    name = dwarf\n",
    "    sigma = properties['sigma']\n",
    "    reff = properties['reff']\n",
    "    sigerr = properties['sigerr']\n",
    "    refferr = properties['refferr']\n",
    "    if sigma is not None and reff is not None:\n",
    "        names.append(name)\n",
    "        sigmas.append(sigma)\n",
    "        reffs.append(reff)\n",
    "        if isinstance(sigerr, list):\n",
    "            sigerr_upper.append(sigerr[1])\n",
    "            sigerr_lower.append(sigerr[0])\n",
    "        else:\n",
    "            sigerr_upper.append(sigerr)\n",
    "            sigerr_lower.append(sigerr)\n",
    "        if isinstance(refferr, list):\n",
    "            refferr_upper.append(refferr[1])\n",
    "            refferr_lower.append(refferr[0])\n",
    "        else:\n",
    "            refferr_upper.append(refferr)\n",
    "            refferr_lower.append(refferr)\n",
    "\n",
    "sigmas= np.array(sigmas)\n",
    "reffs= np.array(reffs) / 1000\n",
    "sigerr_upper = np.array(sigerr_upper)\n",
    "sigerr_lower = np.array(sigerr_lower)\n",
    "refferr_upper = np.array(refferr_upper) / 1000\n",
    "refferr_lower = np.array(refferr_lower) / 1000\n",
    "sigerr = ((sigerr_upper + sigerr_lower) / 2) + 1e-240\n",
    "refferr = ((refferr_upper + refferr_lower) / 2) + 1e-240\n",
    "    \n",
    "# Establish constants\n",
    "scatter_concentration = 0.16\n",
    "scatter_rhalf2D = 0.234\n",
    "scatter_SI = 0.18\n",
    "log10_to_log = np.log(10)\n",
    "cosmoP18 = cosmology.setCosmology('planck18')\n",
    "log10_to_log = 1/np.log(10)\n",
    "tin = float(cosmoP18.age(1))\n",
    "h = cosmoP18.h\n",
    "rhoc_zin = cosmoP18.rho_c(1)*h**2\n",
    "probsamp_data = np.load('probsamp_data_arrays.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5483e",
   "metadata": {},
   "source": [
    "# Define Conditional Probability Functions\n",
    "\n",
    "In the summary of methods and conclusions we go through all of the conditional probabilities required to compute mu. Here we define these functions. Note that we take the log of M, M_star, R_eff, or concentration as the input, and convert it to its non-log counterpart. This improves performance, but note that we need to multiply each probability that takes the log as an input by the exponential of that log value when computing the total probability. Before all of this we define all of the values for the parameters that we will use and compute a few more constants that depend on those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ee18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "trmass = 9\n",
    "alpha=1.31\n",
    "Mhof=10**8.35\n",
    "beta=0.96\n",
    "sigma=0.15\n",
    "gamma=0\n",
    "mcorethres=10**9\n",
    "sigma_CO=1\n",
    "y_c=1\n",
    "M_mw=10**12\n",
    "\n",
    "# Compute some more constants used in functions\n",
    "WDM1 = wdm.WDM(trmass)\n",
    "M_half_mode = dis.dark_matter.models.wdm.helper.half_mode_mass(WDM1.mWDM)\n",
    "Rvir_mw = halo.mass_so.M_to_R(M_mw*cosmoP18.h, 0, '200c') / cosmoP18.h\n",
    "c_mw = 9\n",
    "r_s_mw = Rvir_mw/c_mw\n",
    "\n",
    "# Define conditional probability functions\n",
    "\n",
    "# P_M function\n",
    "def P_M(logM):\n",
    "    \"\"\" Returns the probability to have a halo mass M \"\"\"\n",
    "    M = np.exp(logM)\n",
    "    tffactor = (1 + (4.2*M_half_mode/M)**2.5)**-0.2\n",
    "    hofprob = .5 + .5*spec.erf(alpha*np.log10(M/Mhof))\n",
    "    P_M_value = (M**(-1.84)) * hofprob * tffactor\n",
    "    return P_M_value\n",
    "\n",
    "# P_Reff function\n",
    "def P_Reff(logR_eff, logM_star):\n",
    "    \"\"\" Returns the probability to have log(half-light radius) logR_eff given a log(stellar mass) logM_star \"\"\"\n",
    "    R_eff = np.exp(logR_eff)\n",
    "    M_star = np.exp(logM_star)\n",
    "    scatter=0.234*log10_to_log\n",
    "    mean=np.log(0.0077624712 * (M_star**0.268))\n",
    "    return (1/(R_eff*scatter*(np.sqrt(2*np.pi))))*np.exp((-(np.log(R_eff)-mean)**2)/(2*(scatter**2)))\n",
    "\n",
    "# P_c function\n",
    "\n",
    "# Load the data file only once and set up the 2D interpolator\n",
    "data = np.loadtxt('c200WDM_results.txt')\n",
    "M_values = np.logspace(7, 12, 100)\n",
    "trmass_values = np.linspace(1, 12, 100)\n",
    "interpolator = CartesianGridInterpolator((np.log10(M_values), trmass_values), data)\n",
    "# Function to retrieve the interpolated data point for given trmass and M values\n",
    "def get_c200WDM(trmass_target, M_target):\n",
    "    \"\"\" Returns the concentration corresponding to a thermal mass trmass_target and halo mass M_target \"\"\"\n",
    "    # Use the 2D interpolator to find the c200WDM value for the given trmass and M\n",
    "    c200WDM_value = interpolator([[np.log10(M_target), trmass_target]])[0]\n",
    "    return c200WDM_value\n",
    "\n",
    "def P_c(logc, logM):\n",
    "    \"\"\" Returns the probability to have log(concentration) logc given a halo mass M \"\"\"\n",
    "    M = np.exp(logM)\n",
    "    c = np.exp(logc)\n",
    "    scatter=0.16*log10_to_log\n",
    "    mean = np.log(get_c200WDM(trmass, M))\n",
    "    P_c_value = (1/(c*scatter*(np.sqrt(2*np.pi))))*np.exp((-(np.log(c)-mean)**2)/(2*(scatter**2)))\n",
    "    return P_c_value\n",
    "\n",
    "# P_Mstar function (product of P_cc and P_smhm)\n",
    "r_c_GK_raw, integral_GK_raw = np.loadtxt(\"disSat/data/radial_distributions/gk17-menc.dat\", unpack=True)\n",
    "\n",
    "def nfw(rs, rc, n0):\n",
    "    \"\"\" Returns the integral of r**2 * n_NFW(r), from 0 to rc, where\n",
    "    n_NFW(r) = n0/(r/rs (1+r/rs)**2)\n",
    "    \"\"\"\n",
    "    return (n0 * rs**3) * (np.log((rc/rs) + 1) - (rc/(rc+rs)))\n",
    "\n",
    "def P_cc(logM_star):\n",
    "    \"\"\" Returns the observation probability for log(stellar mass) logM_star \"\"\"\n",
    "    M_star = np.exp(logM_star)\n",
    "    L_WDM = M_star / 2\n",
    "    r_c_WDM = 1.5 * (L_WDM**0.51)\n",
    "    if r_c_WDM < Rvir_mw:\n",
    "        n_NFW = nfw(r_s_mw, r_c_WDM, 1)\n",
    "        C_rbottom_WDM = (1-y_c)*np.interp(r_c_WDM, r_c_GK_raw, integral_GK_raw) + y_c*n_NFW\n",
    "\n",
    "        C_rtop_WDM = nfw(r_s_mw, Rvir_mw, 1)\n",
    "\n",
    "        C_r_WDM = C_rtop_WDM / C_rbottom_WDM\n",
    "    else:\n",
    "        C_r_WDM = 1\n",
    "\n",
    "    Omega = 3.65\n",
    "    C_omega = ((4*np.pi) / Omega)*sigma_CO\n",
    "\n",
    "    C_WDM=C_r_WDM*C_omega\n",
    "    Pobs_WDM=1/C_WDM\n",
    "    threshold = 4e5\n",
    "    if M_star > threshold:\n",
    "           Pobs_WDM=1\n",
    "    return Pobs_WDM\n",
    "\n",
    "def P_smhm(logM_star, logM):\n",
    "    \"\"\" Stellar mass-halo mass relation \"\"\"\n",
    "    M = np.exp(logM)\n",
    "    M_star = np.exp(logM_star)\n",
    "    N = .046\n",
    "    M0 = 1.5e12\n",
    "    mean = np.log(M * N * ((M / M0)**beta))\n",
    "    scatter = sigma + (gamma * np.log10(M/(10**11)))\n",
    "    P_smhm_value = (1 / (M_star * scatter * (np.sqrt(2 * np.pi)))) * np.exp((-(np.log(M_star) - mean)**2) / (2 * (scatter**2)))\n",
    "    return P_smhm_value\n",
    "\n",
    "def P_Mstar(logM_star, logM):\n",
    "    \"\"\" Returns the probability to have log(stellar mass) logM_star given a halo mass M \"\"\"\n",
    "    return P_cc(logM_star)*P_smhm(logM_star, logM)\n",
    "\n",
    "# Make P_tot function\n",
    "def P_tot(logM, logM_star, logR_eff, logc):\n",
    "    \"\"\" Returns the differential probability to have\n",
    "          log(halo mass) logM\n",
    "          log(stellar mass) logM_star\n",
    "          log(half-light radius) logR_eff\n",
    "          log(concentration) logc\n",
    "    \"\"\"\n",
    "    return P_M(logM)*P_Reff(logR_eff, logM_star)*P_c(logc, logM)*P_Mstar(logM_star, logM)*np.exp(logM)*np.exp(logM_star)*np.exp(logc)*np.exp(logR_eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9eb91",
   "metadata": {},
   "source": [
    "# Preparing to Compute Mu\n",
    "\n",
    "Before we can compute mu, we need to do a few things. First, we must create a long list of probabilistically sampled points to train our numerical integrator so that it can perform well. To do this, we will create a function that returns an array of probabilistic sampled halo masses, stellar masses, 2D half-light radii, and concentrations given an initial array of presampled halo masses (just a numpy geomspace). One small thing to note is that the probability of a mass being sampled depends on M<sup>-0.84</sup> in this case instead of M<sup>-1.84</sup>. The next thing we need to do is define the mu integrand, which has a sigma gaussian function and a R_eff gaussian function. The R_eff gaussian function is straightforward to define, but the sigma gaussian is harder because we need to also define some functions to compute the line of sight stellar velocity dispersion, whose definition changes depending on the mass profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f524d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probsamp1 to return sampled arrays of M, m_star, R_eff, and c\n",
    "def P_M_SAMP(logM):\n",
    "    \"\"\" Returns the probability to have a log(halo mass) logM to be used in probabilistic sampling\"\"\"\n",
    "    M=np.exp(logM)\n",
    "    tffactor=WDM1.transfer_function(mass=M, mWDM=WDM1.mWDM)\n",
    "    hofprob = .5 + .5*spec.erf(alpha*np.log10(M/Mhof))\n",
    "    P_M_value = (M**(-.84)) * hofprob * tffactor\n",
    "    return P_M_value\n",
    "\n",
    "def smhm_mean_scatter(logM):\n",
    "    \"\"\"Return the smhm mean(stellar mass) and scatter given a log(halo mass) logM\"\"\"\n",
    "    M = np.exp(logM)\n",
    "    N = .046\n",
    "    M0 = 1.5e12\n",
    "    mean = np.log(M * N * ((M / M0)**beta))\n",
    "    scatter = sigma + (gamma * np.log10(M/(10**11)))\n",
    "    return mean, scatter\n",
    "\n",
    "def probsamp1(M):\n",
    "    \"\"\"\n",
    "    Input: array of presampled mass values (example: M=np.geomspace(1e7,1e12,100000))\n",
    "\n",
    "    Output: array of WDM halo masses, stellar masses, concentrations and 2D halflight radii \n",
    "    values sampled based on subhalo mass function, transfer function, halo occupation fraction,\n",
    "    and observation probability.\n",
    "    \"\"\"\n",
    "    # Compute M200_WDM\n",
    "    Pm = [P_M_SAMP(np.log(M_)) for M_ in M]\n",
    "    Pm /= np.sum(Pm)\n",
    "    M200_WDM = np.random.choice(M, len(M), p=Pm)\n",
    "\n",
    "    # Compute m_stellar\n",
    "    mean_M_star, scatter_M_star = smhm_mean_scatter(np.log(M200_WDM))\n",
    "    m_stellarWDM = np.random.lognormal(mean=mean_M_star, sigma=(scatter_M_star * log10_to_log), size=len(M200_WDM))\n",
    "\n",
    "    # Compute concentration\n",
    "    c200_WDM_median = [get_c200WDM(trmass, M_) for M_ in M]\n",
    "    c200_WDM = np.random.lognormal(mean=np.log(c200_WDM_median), sigma=(scatter_concentration * log10_to_log), size=len(M200_WDM))\n",
    "\n",
    "    # Compute P_obs\n",
    "    Pobs_WDM=[P_cc(np.log(Mstar_)) for Mstar_ in m_stellarWDM]\n",
    "\n",
    "    # Compute R_eff\n",
    "    scatter_R_eff=0.234*log10_to_log\n",
    "    mean_R_eff=np.log(0.0077624712 * m_stellarWDM**0.268)\n",
    "    Reff_WDM = np.random.lognormal(mean=mean_R_eff, sigma=scatter_R_eff, size=len(M200_WDM))\n",
    "\n",
    "    # Sample everything based on observation probability    \n",
    "    z = np.random.rand(len(M200_WDM))\n",
    "    Mfin = M200_WDM[z<Pobs_WDM]\n",
    "    Mstarfin = m_stellarWDM[z<Pobs_WDM]\n",
    "    Refffin = Reff_WDM[z<Pobs_WDM]\n",
    "    cfin = c200_WDM[z<Pobs_WDM]\n",
    "\n",
    "    # Return sampled mass array\n",
    "    return Mfin, Mstarfin, Refffin, cfin\n",
    "\n",
    "# Create a function to give sigLOS given WDM halo mass, density profile, concentration, stellar mass, and halflight radius\n",
    "def find_normalization_constant(M_halo, r_halo, rs):\n",
    "    \"\"\"Find the normalization constant n0 given the mass of the halo and its radius.\"\"\"\n",
    "    term1 = np.log((r_halo / rs) + 1)\n",
    "    term2 = r_halo / (r_halo + rs)\n",
    "    n0 = M_halo / (rs**3 * (term1 - term2))\n",
    "    return n0\n",
    "\n",
    "def menc_nfw(rs, rc, M_halo, r_halo):\n",
    "    return nfw(rs, rc, find_normalization_constant(M_halo, r_halo, rs))\n",
    "\n",
    "def menc_corenfw(rs, rhalf, halomass, rvir):\n",
    "    G = 4.5171031e-39 \n",
    "    ETA,KAPPA = 3.,0.04\n",
    "    #fCORENFW = lambda x: (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))  # x = r/rc\n",
    "    fCORENFW = lambda x: np.tanh(x)\n",
    "    GYR = 3600*24*365.25*1e9 # seconds in a Gyr\n",
    "    tSF = tin\n",
    "    tSF *= GYR\n",
    "    tDYN = 2*np.pi*np.sqrt((rs)**3/G/(menc_nfw(rs, rhalf, halomass, rvir)))\n",
    "    q = KAPPA * tSF / tDYN\n",
    "    n = fCORENFW(q)\n",
    "    Rc = ETA * rhalf * 0.75  # coreNFW core radius, in kpc\n",
    "    suppression = fCORENFW(rhalf/Rc)**n\n",
    "    return menc_nfw(rs, rhalf, halomass, rvir)*suppression\n",
    "\n",
    "def get_virial_radius_at_infall(M):\n",
    "    return (M/(4/3*np.pi*200*rhoc_zin))**(1./3.)\n",
    "\n",
    "def sigLOS(halomass, concentration, r_eff, M_star):\n",
    "    G = 4.3009e-3 / 1000  #kpc * (km/s)^2 * (Msun)^-1\n",
    "    rvir = get_virial_radius_at_infall(halomass)\n",
    "    rs = rvir / concentration\n",
    "    rhalf = r_eff / 0.75\n",
    "    if halomass < mcorethres:\n",
    "        Menc = menc_nfw(rs, rhalf, halomass, rvir)\n",
    "    else:\n",
    "        Menc = menc_corenfw(rs, rhalf, halomass, rvir)\n",
    "    sigLOSvalue = np.sqrt((G*(Menc+(M_star/2)))/(4*r_eff))\n",
    "    return sigLOSvalue\n",
    "\n",
    "# Compute the mu integrand\n",
    "def sigma_gaus(logM, logM_star, logR_eff, logc, siglosobs, deltasigma):\n",
    "    \"\"\"The gaussian for sigLOS in equation B2\"\"\"\n",
    "    M = np.exp(logM)\n",
    "    M_star = np.exp(logM_star)\n",
    "    R_eff = np.exp(logR_eff)\n",
    "    c = np.exp(logc)\n",
    "    if M < mcorethres:\n",
    "        prof='NFW'\n",
    "    else:\n",
    "        prof='coreNFW'\n",
    "    siglos = sigLOS(M, c, R_eff, M_star)\n",
    "    return (1/(np.sqrt(2*np.pi)*deltasigma))*np.exp((-(siglosobs-siglos)**2)/(2*(deltasigma**2)))\n",
    "\n",
    "def reff_gaus(logR_eff, reffobs, deltareff):\n",
    "    \"\"\"The gaussian for R_eff in equation B2\"\"\"\n",
    "    R_eff = np.exp(logR_eff)\n",
    "    return (1/(np.sqrt(2*np.pi)*deltareff))*np.exp((-(reffobs-R_eff)**2)/(2*(deltareff**2)))\n",
    "\n",
    "def mu_integrand(logM, logM_star, logR_eff, logc, siglosobs, reffobs, deltasigma, deltareff):\n",
    "    \"\"\"Returns the integrand of mu to be integrated\"\"\"\n",
    "    return P_tot(logM, logM_star, logR_eff, logc)*sigma_gaus(logM, logM_star, logR_eff, logc, siglosobs, deltasigma)*reff_gaus(logR_eff, reffobs, deltareff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46952f33",
   "metadata": {},
   "source": [
    "# Integrating to Compute Mu\n",
    "\n",
    "Now, we use the vegas package to integrate. This is the most finicky part of the whole process, as it is extremely dependent on how the integrator is trained. Normally we would just compute a large set of probabilistically sampled points and train the integrator that way, but some issues arrise as small values of y_C for galaxies with small R_eff that increase integration errors. To account for this, we will compute a different array of probabilistically sampled values for galaxies with small R_eff. The integrator is trained using a map of probabilities associated with the input variables. The values of nitn and neval were found to be somewhat optimal to give the best results in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c188361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define integration function to compute mu\n",
    "def integratemu(siglosobs, reffobs, deltasigma, deltareff):\n",
    "    \"\"\" Returns mu given siglosobs, reffobs, deltasigma, and deltareff \"\"\"\n",
    "    if reffobs < .04:\n",
    "        M, m_star, R_eff, c = probsamp1(np.geomspace(1e7,1e8,2000000))\n",
    "    else:\n",
    "        M, m_star, R_eff, c = probsamp1(np.geomspace(1e6,1e13,200000))\n",
    "    x = np.stack((np.log(M), np.log(m_star), np.log(R_eff), np.log(c)), axis=-1)\n",
    "    delta_range = 3\n",
    "    reff_range = [reffobs - delta_range*deltareff, reffobs + delta_range*deltareff]\n",
    "    map = vegas.AdaptiveMap([np.log(np.array([1e7,3e10])), np.log(np.array([1e2,5e9])), np.log(np.array(reff_range)), np.log(np.array([1, 12]))])\n",
    "    probs1 = np.array([])\n",
    "    for k in range(len(x)):\n",
    "        prob1 = (mu_integrand(x[k][0],x[k][1],x[k][2],x[k][3],siglosobs,reffobs,deltasigma,deltareff))\n",
    "        probs1 = np.append(probs1, prob1)\n",
    "    def integrand1(x):\n",
    "        return (mu_integrand(x[0],x[1],x[2],x[3],siglosobs,reffobs,deltasigma,deltareff))\n",
    "    map.adapt_to_samples(x, probs1, nitn=6)\n",
    "    integ = vegas.Integrator(map, alpha=0.03)\n",
    "    r1 = integ(integrand1, neval=20000, nitn=6)\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6507d3",
   "metadata": {},
   "source": [
    "# Computing the Probability to Observe N_obs Galaxies\n",
    "\n",
    "We need to compute the normalization factor of mu which will give us the predicted number of observed galaxies. We do this using scipy's dblquad function. Then, we can compute the probability to observe N_obs number of observed galaxies (which is given by a negative binomial distribution) by defining a series of simple functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e915255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalization of mu values\n",
    "def munormintegrand(logM_star, logM):\n",
    "    \"\"\"Returns the normalization of mu given logM, logM_star, logR_eff, logc\"\"\"\n",
    "    return 0.000854*0.8*M_mw*P_M(logM)*P_Mstar(logM_star, logM)*np.exp(logM)*np.exp(logM_star)\n",
    "\n",
    "a = np.log(1e7)\n",
    "b = np.log(1e12)\n",
    "def smhm(logM):\n",
    "    M = np.exp(logM)\n",
    "    N = 0.046\n",
    "    M0 = 1.5e12\n",
    "    mean = np.log(M * N * ((M / M0)**beta))\n",
    "    return mean\n",
    "c = smhm(a)\n",
    "d = smhm(b)\n",
    "munorm, munormerr = integ.dblquad(munormintegrand, a, b, c, d)\n",
    "\n",
    "# Define negative binomial distribution to compute P(N_obs)\n",
    "def fGAMMA(x):\n",
    "    \"\"\"The gamma function of a negative binomial distribution\"\"\"\n",
    "    return spec.gamma(x)\n",
    "def p_negbin(Nmean, scatter):\n",
    "    \"\"\"The p parameter of a negative binomial distribution\"\"\"\n",
    "    return (1/(1+(scatter*Nmean)))\n",
    "def r_negbin(scatter):\n",
    "    \"\"\"The r parameter of a negative binomial distribution\"\"\"\n",
    "    return 1/scatter\n",
    "def negativebinomial(N, Nmean, scatter):\n",
    "    \"\"\"Returns the negative binomial distribution given a mean and intrinsic scatter\"\"\"\n",
    "    p = p_negbin(Nmean, scatter)\n",
    "    r = r_negbin(scatter)\n",
    "    return ((fGAMMA(N + r) / (fGAMMA(r)*fGAMMA(N+1)))*(p**r)*((1-p)**N))\n",
    "\n",
    "# Compute mu normalization, N_obs, and P(N_obs)\n",
    "N = len(sigmas)\n",
    "N_obs_error = munormerr\n",
    "N_obs = munorm\n",
    "Nmean = N_obs\n",
    "P_Nobs = negativebinomial(N, Nmean, scatter_SI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a1efc",
   "metadata": {},
   "source": [
    "# Compute Mu and -2lnL\n",
    "\n",
    "We iterate through our arrays of data values for observed satellie galaxy properties to compute mu. Then, we compute the likelihood. In this case we also keep track of errors values to be safe. Note that this likelihood values is wrong because we commented out many of the galaxies in our data array for the sake of this example not taking too long to compute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b665f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Likelihood value is: 152.36803876213608\n"
     ]
    }
   ],
   "source": [
    "# Compute mu_i values\n",
    "mu_i = []\n",
    "mu_i_error = []\n",
    "for i in range(len(sigmas)):\n",
    "    mu_i_val = integratemu(sigmas[i],reffs[i],sigerr[i],refferr[i])\n",
    "    muimeanval = mu_i_val.mean\n",
    "    muierrorval = mu_i_val.sdev\n",
    "    mu_i.append(muimeanval)\n",
    "    mu_i_error.append(muierrorval)\n",
    "mu_i = np.array(mu_i) / munorm\n",
    "\n",
    "# Compute neg2ln(L)\n",
    "neg2lnL = -2*np.sum(np.log(mu_i)) - 2*np.log(P_Nobs) + ((sigma_CO-1)**2 / (0.19**2))\n",
    "print('The Likelihood value is:', neg2lnL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4b58f",
   "metadata": {},
   "source": [
    "# Create a Function That Does it All!\n",
    "\n",
    "Now that we know how the process works, let's make a function that takes all model parameters as inputs and computes the likelihood. So that this can be easily copied and pasted for testing, I will also include the introductory parts such as imports and data blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f609777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT BLOCK\n",
    "\n",
    "import numpy as np\n",
    "import scipy.integrate as integ\n",
    "import scipy.special as spec\n",
    "import scipy.optimize as optim\n",
    "from scipy.ndimage import map_coordinates\n",
    "import scipy.optimize as optim\n",
    "import vegas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import disSat as dis\n",
    "import disSat.dark_matter.models.wdm as wdm\n",
    "from colossus.cosmology import cosmology\n",
    "import colossus.halo as halo\n",
    "\n",
    "class CartesianGridInterpolator:\n",
    "    \"\"\"\n",
    "    This class works as scipy.interpolate.RegularGridInterpolator, but\n",
    "    it's optimized for equally-spaced grids.\n",
    "\n",
    "    Obtained from\n",
    "    https://docs.scipy.org/doc/scipy/tutorial/interpolate/ND_regular_grid.html\n",
    "    \"\"\"\n",
    "    def __init__(self, points, values, method='linear'):\n",
    "        self.limits = np.array([[min(x), max(x)] for x in points])\n",
    "        self.values = np.asarray(values, dtype=float)\n",
    "        self.order = {'linear': 1, 'cubic': 3, 'quintic': 5}[method]\n",
    "\n",
    "    def __call__(self, xi):\n",
    "        \"\"\"\n",
    "        `xi` here is an array-like (an array or a list) of points.\n",
    "\n",
    "        Each \"point\" is an ndim-dimensional array_like, representing\n",
    "        the coordinates of a point in ndim-dimensional space.\n",
    "        \"\"\"\n",
    "        # transpose the xi array into the ``map_coordinates`` convention\n",
    "        # which takes coordinates of a point along columns of a 2D array.\n",
    "        xi = np.asarray(xi).T\n",
    "\n",
    "        # convert from data coordinates to pixel coordinates\n",
    "        ns = self.values.shape\n",
    "        coords = [(n-1)*(val - lo) / (hi - lo)\n",
    "                  for val, n, (lo, hi) in zip(xi, ns, self.limits)]\n",
    "\n",
    "        # interpolate\n",
    "        return map_coordinates(self.values, coords,\n",
    "                               order=self.order,\n",
    "                               cval=np.nan)  # fill_value\n",
    "    \n",
    "# Subhalo data block\n",
    "\n",
    "dwarfs_updated = {'.Boötes I': {'reff': 191.0,'refferr': [5.0, 5.0],'sigma': 5.1,'sigerr': [0.8, 0.7]},\n",
    "                   '.Boötes II': {'reff': 38.7,'refferr': [5.1, 5.1],'sigma': 2.9,'sigerr': [1.2, 1.6]},\n",
    "                  '.Canes Venatici I': {'reff': 452.0,'refferr': [13.0, 13.0],'sigma': 7.6,'sigerr': [0.4, 0.4]},\n",
    "                  '.Canes Venatici II': {'reff': 70.7,'refferr': [11.2, 11.2],'sigma': 4.6,'sigerr': [1.0, 1.0]},\n",
    "                  '.Carina': {'reff': 308.0,'refferr': [3.0, 3.0],'sigma': 6.6,'sigerr': [1.2, 1.2]},\n",
    "                  '.Coma Berenices': {'reff': 72.1,'refferr': [3.8, 3.8],'sigma': 4.6,'sigerr': [0.8, 0.8]},\n",
    "                  '.Draco': {'reff': 214.0,'refferr': [2.0, 2.0],'sigma': 9.1,'sigerr': [1.2, 1.2]},\n",
    "                  '.Fornax': {'reff': 838.0,'refferr': [3.0, 3.0],'sigma': 11.7,'sigerr': [0.9, 0.9]},\n",
    "                  '.Hercules': {'reff': 216.0,'refferr': [17.0, 17.0],'sigma': 5.1,'sigerr': [0.9, 0.9]},\n",
    "                  '.Leo I': {'reff': 270.0,'refferr': [2, 2],'sigma': 9.2,'sigerr': [0.4, 0.4]},\n",
    "                  '.Leo II': {'reff': 171.0,'refferr': [2.0, 2.0],'sigma': 7.4,'sigerr': [0.4, 0.4]},\n",
    "                  '.Leo IV': {'reff': 114.0,'refferr': [13.0, 13.0],'sigma': 3.4,'sigerr': [0.9, 1.3]},\n",
    "                  '.Leo V': {'reff': 49.0,'refferr': [16.0, 16.0],'sigma': 2.3,'sigerr': [1.6, 3.2]},\n",
    "                  '.Sculptor': {'reff': 280.0,'refferr': [1.0, 1.0],'sigma': 9.2,'sigerr': [1.1, 1.1]},\n",
    "                  '.Segue 1': {'reff': 24.2,'refferr': [2.8, 2.8],'sigma': 3.7,'sigerr': [1.1, 1.4]},\n",
    "                  '.Sextans': {'reff': 413.0,'refferr': [3.0, 3.0],'sigma': 7.9,'sigerr': [1.3, 1.3]},\n",
    "                  '.Ursa Major I': {'reff': 234.0,'refferr': [10.0, 10.0],'sigma': 7.0,'sigerr': [1.0, 1.0]},\n",
    "                  '.Ursa Major II': {'reff': 128.0,'refferr': [5.0, 5.0],'sigma': 5.6,'sigerr': [1.4, 1.4]},\n",
    "                  '.Ursa Minor': {'reff': 405.0,'refferr': [21.0, 21.0],'sigma': 9.5,'sigerr': [1.2, 1.2]},\n",
    "                  '.Willman 1': {'reff': 27.7,'refferr': [2.4, 2.4],'sigma': 4.0,'sigerr': [0.8, 0.8]}\n",
    "                 }\n",
    "\n",
    "sigmas = []\n",
    "reffs = []\n",
    "sigerr_upper = []\n",
    "sigerr_lower = []\n",
    "refferr_upper = []\n",
    "refferr_lower = []\n",
    "names = []\n",
    "\n",
    "for dwarf, properties in dwarfs_updated.items():\n",
    "    name = dwarf\n",
    "    sigma = properties['sigma']\n",
    "    reff = properties['reff']\n",
    "    sigerr = properties['sigerr']\n",
    "    refferr = properties['refferr']\n",
    "    if sigma is not None and reff is not None:\n",
    "        names.append(name)\n",
    "        sigmas.append(sigma)\n",
    "        reffs.append(reff)\n",
    "        if isinstance(sigerr, list):\n",
    "            sigerr_upper.append(sigerr[1])\n",
    "            sigerr_lower.append(sigerr[0])\n",
    "        else:\n",
    "            sigerr_upper.append(sigerr)\n",
    "            sigerr_lower.append(sigerr)\n",
    "        if isinstance(refferr, list):\n",
    "            refferr_upper.append(refferr[1])\n",
    "            refferr_lower.append(refferr[0])\n",
    "        else:\n",
    "            refferr_upper.append(refferr)\n",
    "            refferr_lower.append(refferr)\n",
    "\n",
    "sigmas= np.array(sigmas)\n",
    "reffs= np.array(reffs) / 1000\n",
    "sigerr_upper = np.array(sigerr_upper)\n",
    "sigerr_lower = np.array(sigerr_lower)\n",
    "refferr_upper = np.array(refferr_upper) / 1000\n",
    "refferr_lower = np.array(refferr_lower) / 1000\n",
    "sigerr = ((sigerr_upper + sigerr_lower) / 2) + 1e-240\n",
    "refferr = ((refferr_upper + refferr_lower) / 2) + 1e-240\n",
    "    \n",
    "# Establish constants\n",
    "scatter_concentration = 0.16\n",
    "scatter_rhalf2D = 0.234\n",
    "scatter_SI = 0.18\n",
    "log10_to_log = np.log(10)\n",
    "cosmoP18 = cosmology.setCosmology('planck18')\n",
    "log10_to_log = 1/np.log(10)\n",
    "tin = float(cosmoP18.age(1))\n",
    "h = cosmoP18.h\n",
    "rhoc_zin = cosmoP18.rho_c(1)*h**2\n",
    "probsamp_data = np.load('probsamp_data_arrays.npz')\n",
    "\n",
    "def neg2lnL(trmass=9,alpha=1.31,Mhof=10**8.35,beta=0.96,sigma=0.15,gamma=0,mcorethres=10**9,sigma_CO=1,y_c=1,M_mw=10**12):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "    \n",
    "    1. trmass (mass of WDM particles) (default: 9keV)\n",
    "    2. alpha (the halo occupation fraction parameter) (default: 1.31)\n",
    "    3. Mhof (the halao occupation fraction parameter) (default: 10**8.35)\n",
    "    4. beta (the power law relation for the SMHM relation) (default: 0.96)\n",
    "    5. sigma (a constant in the scatter of the SMHM relation) (default: 0.15)\n",
    "    6. gamma (a constant in the scatter of the SMHM relation) (default: 0)\n",
    "    7. mcorethres (the mass where the density profile of a galaxy switches from NFW to coreNFW) (default: 10**9)\n",
    "    8. sigma_CO (included to account for the anisotropy of of the satellite distribution) (default: 1)\n",
    "    9. y_c (interpolates between NFW and coreNFW profiles to account for tidal disruption) (default: 1)\n",
    "    10. M_mw (mass of Milky Way galaxy + halo) (default: 10**12)\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    -2ln(L) which is the likelihood function to obtain subhalos with the given input parameters\n",
    "    \"\"\"\n",
    "    # Compute some starter constants that rely on initial parameters\n",
    "    WDM1 = wdm.WDM(trmass)\n",
    "    M_half_mode = dis.dark_matter.models.wdm.helper.half_mode_mass(WDM1.mWDM)\n",
    "    Rvir_mw = halo.mass_so.M_to_R(M_mw*cosmoP18.h, 0, '200c') / cosmoP18.h\n",
    "    c_mw = 9\n",
    "    r_s_mw = Rvir_mw/c_mw\n",
    "\n",
    "    # MAKE INTEGRATION FUNCTIONS\n",
    "    \n",
    "    def P_M(logM):\n",
    "        \"\"\" Returns the probability to have a halo mass M \"\"\"\n",
    "        M = np.exp(logM)\n",
    "        tffactor = (1 + (4.2*M_half_mode/M)**2.5)**-0.2\n",
    "        hofprob = .5 + .5*spec.erf(alpha*np.log10(M/Mhof))\n",
    "        P_M_value = (M**(-1.84)) * hofprob * tffactor\n",
    "        return P_M_value\n",
    "\n",
    "    def P_Reff(logR_eff, logM_star):\n",
    "        \"\"\" Returns the probability to have log(half-light radius) logR_eff given a log(stellar mass) logM_star \"\"\"\n",
    "        R_eff = np.exp(logR_eff)\n",
    "        M_star = np.exp(logM_star)\n",
    "        scatter=0.234*log10_to_log\n",
    "        mean=np.log(0.0077624712 * (M_star**0.268))\n",
    "        return (1/(R_eff*scatter*(np.sqrt(2*np.pi))))*np.exp((-(np.log(R_eff)-mean)**2)/(2*(scatter**2)))\n",
    "\n",
    "    # Make function for P(c)\n",
    "\n",
    "    # Load the data file only once and set up the 2D interpolator\n",
    "    data = np.loadtxt('c200WDM_results.txt')\n",
    "    M_values = np.logspace(7, 12, 100)\n",
    "    trmass_values = np.linspace(1, 12, 100)\n",
    "    interpolator = CartesianGridInterpolator((np.log10(M_values), trmass_values), data)\n",
    "    # Function to retrieve the interpolated data point for given trmass and M values\n",
    "    def get_c200WDM(trmass_target, M_target):\n",
    "        \"\"\" Returns the concentration corresponding to a thermal mass trmass_target and halo mass M_target \"\"\"\n",
    "        # Use the 2D interpolator to find the c200WDM value for the given trmass and M\n",
    "        c200WDM_value = interpolator([[np.log10(M_target), trmass_target]])[0]\n",
    "        return c200WDM_value\n",
    "\n",
    "    def P_c(logc, logM):\n",
    "        \"\"\" Returns the probability to have log(concentration) logc given a halo mass M \"\"\"\n",
    "        M = np.exp(logM)\n",
    "        c = np.exp(logc)\n",
    "        scatter=0.16*log10_to_log\n",
    "        mean = np.log(get_c200WDM(trmass, M))\n",
    "        P_c_value = (1/(c*scatter*(np.sqrt(2*np.pi))))*np.exp((-(np.log(c)-mean)**2)/(2*(scatter**2)))\n",
    "        return P_c_value\n",
    "\n",
    "    # Make function for P(M*)\n",
    "    r_c_GK_raw, integral_GK_raw = np.loadtxt(\"disSat/data/radial_distributions/gk17-menc.dat\", unpack=True)\n",
    "\n",
    "    def nfw(rs, rc, n0):\n",
    "        \"\"\" Returns the integral of r**2 * n_NFW(r), from 0 to rc, where\n",
    "        n_NFW(r) = n0/(r/rs (1+r/rs)**2)\n",
    "        \"\"\"\n",
    "        return (n0 * rs**3) * (np.log((rc/rs) + 1) - (rc/(rc+rs)))\n",
    "\n",
    "    def P_cc(logM_star):\n",
    "        \"\"\" Returns the observation probability for log(stellar mass) logM_star \"\"\"\n",
    "        M_star = np.exp(logM_star)\n",
    "        L_WDM = M_star / 2\n",
    "        r_c_WDM = 1.5 * (L_WDM**0.51)\n",
    "        if r_c_WDM < Rvir_mw:\n",
    "            n_NFW = nfw(r_s_mw, r_c_WDM, 1)\n",
    "            C_rbottom_WDM = (1-y_c)*np.interp(r_c_WDM, r_c_GK_raw, integral_GK_raw) + y_c*n_NFW\n",
    "\n",
    "            C_rtop_WDM = nfw(r_s_mw, Rvir_mw, 1)\n",
    "\n",
    "            C_r_WDM = C_rtop_WDM / C_rbottom_WDM\n",
    "        else:\n",
    "            C_r_WDM = 1\n",
    "\n",
    "        Omega = 3.65\n",
    "        C_omega = ((4*np.pi) / Omega)*sigma_CO\n",
    "\n",
    "        C_WDM=C_r_WDM*C_omega\n",
    "        Pobs_WDM=1/C_WDM\n",
    "        threshold = 4e5\n",
    "        if M_star > threshold:\n",
    "               Pobs_WDM=1\n",
    "        return Pobs_WDM\n",
    "\n",
    "    def P_smhm(logM_star, logM):\n",
    "        \"\"\" Stellar mass-halo mass relation \"\"\"\n",
    "        M = np.exp(logM)\n",
    "        M_star = np.exp(logM_star)\n",
    "        N = .046\n",
    "        M0 = 1.5e12\n",
    "        mean = np.log(M * N * ((M / M0)**beta))\n",
    "        scatter = sigma + (gamma * np.log10(M/(10**11)))\n",
    "        P_smhm_value = (1 / (M_star * scatter * (np.sqrt(2 * np.pi)))) * np.exp((-(np.log(M_star) - mean)**2) / (2 * (scatter**2)))\n",
    "        return P_smhm_value\n",
    "\n",
    "    def P_Mstar(logM_star, logM):\n",
    "        \"\"\" Returns the probability to have log(stellar mass) logM_star given a halo mass M \"\"\"\n",
    "        return P_cc(logM_star)*P_smhm(logM_star, logM)\n",
    "\n",
    "    # Make P_tot function\n",
    "    def P_tot(logM, logM_star, logR_eff, logc):\n",
    "        \"\"\" Returns the differential probability to have\n",
    "              log(halo mass) logM\n",
    "              log(stellar mass) logM_star\n",
    "              log(half-light radius) logR_eff\n",
    "              log(concentration) logc\n",
    "        \"\"\"\n",
    "        return P_M(logM)*P_Reff(logR_eff, logM_star)*P_c(logc, logM)*P_Mstar(logM_star, logM)*np.exp(logM)*np.exp(logM_star)*np.exp(logc)*np.exp(logR_eff)\n",
    "\n",
    "    # Create probsamp1 to return sampled arrays of M, m_star, R_eff, and c\n",
    "    def P_M_SAMP(logM):\n",
    "        \"\"\" Returns the probability to have a log(halo mass) logM to be used in probabilistic sampling\"\"\"\n",
    "        M=np.exp(logM)\n",
    "        tffactor=WDM1.transfer_function(mass=M, mWDM=WDM1.mWDM)\n",
    "        hofprob = .5 + .5*spec.erf(alpha*np.log10(M/Mhof))\n",
    "        P_M_value = (M**(-.84)) * hofprob * tffactor\n",
    "        return P_M_value\n",
    "\n",
    "    def smhm_mean_scatter(logM):\n",
    "        \"\"\"Return the smhm mean(stellar mass) and scatter given a log(halo mass) logM\"\"\"\n",
    "        M = np.exp(logM)\n",
    "        N = .046\n",
    "        M0 = 1.5e12\n",
    "        mean = np.log(M * N * ((M / M0)**beta))\n",
    "        scatter = sigma + (gamma * np.log10(M/(10**11)))\n",
    "        return mean, scatter\n",
    "\n",
    "    def probsamp1(M):\n",
    "        \"\"\"\n",
    "        Input: array of presampled mass values (example: M=np.geomspace(1e7,1e12,100000))\n",
    "\n",
    "        Output: array of WDM halo masses, stellar masses, concentrations and 2D halflight radii \n",
    "        values sampled based on subhalo mass function, transfer function, halo occupation fraction,\n",
    "        and observation probability.\n",
    "        \"\"\"\n",
    "        # Compute M200_WDM\n",
    "        Pm = [P_M_SAMP(np.log(M_)) for M_ in M]\n",
    "        Pm /= np.sum(Pm)\n",
    "        M200_WDM = np.random.choice(M, len(M), p=Pm)\n",
    "\n",
    "        # Compute m_stellar\n",
    "        mean_M_star, scatter_M_star = smhm_mean_scatter(np.log(M200_WDM))\n",
    "        m_stellarWDM = np.random.lognormal(mean=mean_M_star, sigma=(scatter_M_star * log10_to_log), size=len(M200_WDM))\n",
    "\n",
    "        # Compute concentration\n",
    "        c200_WDM_median = [get_c200WDM(trmass, M_) for M_ in M]\n",
    "        c200_WDM = np.random.lognormal(mean=np.log(c200_WDM_median), sigma=(scatter_concentration * log10_to_log), size=len(M200_WDM))\n",
    "\n",
    "        # Compute P_obs\n",
    "        Pobs_WDM=[P_cc(np.log(Mstar_)) for Mstar_ in m_stellarWDM]\n",
    "\n",
    "        # Compute R_eff\n",
    "        scatter_R_eff=0.234*log10_to_log\n",
    "        mean_R_eff=np.log(0.0077624712 * m_stellarWDM**0.268)\n",
    "        Reff_WDM = np.random.lognormal(mean=mean_R_eff, sigma=scatter_R_eff, size=len(M200_WDM))\n",
    "\n",
    "        # Sample everything based on observation probability    \n",
    "        z = np.random.rand(len(M200_WDM))\n",
    "        Mfin = M200_WDM[z<Pobs_WDM]\n",
    "        Mstarfin = m_stellarWDM[z<Pobs_WDM]\n",
    "        Refffin = Reff_WDM[z<Pobs_WDM]\n",
    "        cfin = c200_WDM[z<Pobs_WDM]\n",
    "\n",
    "        # Return sampled mass array\n",
    "        return Mfin, Mstarfin, Refffin, cfin\n",
    "\n",
    "    # Create a function to give sigLOS given WDM halo mass, density profile, concentration, stellar mass, and halflight radius\n",
    "    def find_normalization_constant(M_halo, r_halo, rs):\n",
    "        \"\"\"Find the normalization constant n0 given the mass of the halo and its radius.\"\"\"\n",
    "        term1 = np.log((r_halo / rs) + 1)\n",
    "        term2 = r_halo / (r_halo + rs)\n",
    "        n0 = M_halo / (rs**3 * (term1 - term2))\n",
    "        return n0\n",
    "\n",
    "    def menc_nfw(rs, rc, M_halo, r_halo):\n",
    "        return nfw(rs, rc, find_normalization_constant(M_halo, r_halo, rs))\n",
    "\n",
    "    def menc_corenfw(rs, rhalf, halomass, rvir):\n",
    "        G = 4.5171031e-39 \n",
    "        ETA,KAPPA = 3.,0.04\n",
    "        #fCORENFW = lambda x: (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))  # x = r/rc\n",
    "        fCORENFW = lambda x: np.tanh(x)\n",
    "        GYR = 3600*24*365.25*1e9 # seconds in a Gyr\n",
    "        tSF = tin\n",
    "        tSF *= GYR\n",
    "        tDYN = 2*np.pi*np.sqrt((rs)**3/G/(menc_nfw(rs, rhalf, halomass, rvir)))\n",
    "        q = KAPPA * tSF / tDYN\n",
    "        n = fCORENFW(q)\n",
    "        Rc = ETA * rhalf * 0.75  # coreNFW core radius, in kpc\n",
    "        suppression = fCORENFW(rhalf/Rc)**n\n",
    "        return menc_nfw(rs, rhalf, halomass, rvir)*suppression\n",
    "\n",
    "    def get_virial_radius_at_infall(M):\n",
    "        return (M/(4/3*np.pi*200*rhoc_zin))**(1./3.)\n",
    "\n",
    "    def sigLOS(halomass, concentration, r_eff, M_star):\n",
    "        G = 4.3009e-3 / 1000  #kpc * (km/s)^2 * (Msun)^-1\n",
    "        rvir = get_virial_radius_at_infall(halomass)\n",
    "        rs = rvir / concentration\n",
    "        rhalf = r_eff / 0.75\n",
    "        if halomass < mcorethres:\n",
    "            Menc = menc_nfw(rs, rhalf, halomass, rvir)\n",
    "        else:\n",
    "            Menc = menc_corenfw(rs, rhalf, halomass, rvir)\n",
    "        sigLOSvalue = np.sqrt((G*(Menc+(M_star/2)))/(4*r_eff))\n",
    "        return sigLOSvalue\n",
    "\n",
    "    # Compute the mu integrand\n",
    "    def sigma_gaus(logM, logM_star, logR_eff, logc, siglosobs, deltasigma):\n",
    "        \"\"\"The gaussian for sigLOS in equation B2\"\"\"\n",
    "        M = np.exp(logM)\n",
    "        M_star = np.exp(logM_star)\n",
    "        R_eff = np.exp(logR_eff)\n",
    "        c = np.exp(logc)\n",
    "        if M < mcorethres:\n",
    "            prof='NFW'\n",
    "        else:\n",
    "            prof='coreNFW'\n",
    "        siglos = sigLOS(M, c, R_eff, M_star)\n",
    "        return (1/(np.sqrt(2*np.pi)*deltasigma))*np.exp((-(siglosobs-siglos)**2)/(2*(deltasigma**2)))\n",
    "\n",
    "    def reff_gaus(logR_eff, reffobs, deltareff):\n",
    "        \"\"\"The gaussian for R_eff in equation B2\"\"\"\n",
    "        R_eff = np.exp(logR_eff)\n",
    "        return (1/(np.sqrt(2*np.pi)*deltareff))*np.exp((-(reffobs-R_eff)**2)/(2*(deltareff**2)))\n",
    "\n",
    "    def mu_integrand(logM, logM_star, logR_eff, logc, siglosobs, reffobs, deltasigma, deltareff):\n",
    "        \"\"\"Returns the integrand of mu to be integrated\"\"\"\n",
    "        return P_tot(logM, logM_star, logR_eff, logc)*sigma_gaus(logM, logM_star, logR_eff, logc, siglosobs, deltasigma)*reff_gaus(logR_eff, reffobs, deltareff)\n",
    "\n",
    "    # Compute probabilistically sampled points to train vegas using probsamp1\n",
    "\n",
    "    \n",
    "    # Define integration function to compute mu\n",
    "    def integratemu(siglosobs, reffobs, deltasigma, deltareff):\n",
    "        \"\"\" Returns mu given siglosobs, reffobs, deltasigma, and deltareff \"\"\"\n",
    "        if reffobs < .04:\n",
    "            M, m_star, R_eff, c = probsamp1(np.geomspace(1e7,1e8,2000000))\n",
    "        else:\n",
    "            M, m_star, R_eff, c = probsamp1(np.geomspace(1e6,1e13,200000))\n",
    "        x = np.stack((np.log(M), np.log(m_star), np.log(R_eff), np.log(c)), axis=-1)\n",
    "        delta_range = 3\n",
    "        reff_range = [reffobs - delta_range*deltareff, reffobs + delta_range*deltareff]\n",
    "        map = vegas.AdaptiveMap([np.log(np.array([1e7,3e10])), np.log(np.array([1e2,5e9])), np.log(np.array(reff_range)), np.log(np.array([1, 12]))])\n",
    "        probs1 = np.array([])\n",
    "        for k in range(len(x)):\n",
    "            prob1 = (mu_integrand(x[k][0],x[k][1],x[k][2],x[k][3],siglosobs,reffobs,deltasigma,deltareff))\n",
    "            probs1 = np.append(probs1, prob1)\n",
    "        def integrand1(x):\n",
    "            return (mu_integrand(x[0],x[1],x[2],x[3],siglosobs,reffobs,deltasigma,deltareff))\n",
    "        map.adapt_to_samples(x, probs1, nitn=6)\n",
    "        integ = vegas.Integrator(map, alpha=0.03)\n",
    "        r1 = integ(integrand1, neval=20000, nitn=6)\n",
    "        return r1\n",
    "    \n",
    "    # Compute the normalization of mu values\n",
    "    def munormintegrand(logM_star, logM):\n",
    "        \"\"\"Returns the normalization of mu given logM, logM_star, logR_eff, logc\"\"\"\n",
    "        return 0.000854*0.8*M_mw*P_M(logM)*P_Mstar(logM_star, logM)*np.exp(logM)*np.exp(logM_star)\n",
    "    \n",
    "    a = np.log(1e7)\n",
    "    b = np.log(1e12)\n",
    "    def smhm(logM):\n",
    "        M = np.exp(logM)\n",
    "        N = 0.046\n",
    "        M0 = 1.5e12\n",
    "        mean = np.log(M * N * ((M / M0)**beta))\n",
    "        return mean\n",
    "    c = smhm(a)\n",
    "    d = smhm(b)\n",
    "    munorm, munormerr = integ.dblquad(munormintegrand, a, b, c, d)\n",
    "    \n",
    "    # Define negative binomial distribution to compute P(N_obs)\n",
    "    def fGAMMA(x):\n",
    "        \"\"\"The gamma function of a negative binomial distribution\"\"\"\n",
    "        return spec.gamma(x)\n",
    "    def p_negbin(Nmean, scatter):\n",
    "        \"\"\"The p parameter of a negative binomial distribution\"\"\"\n",
    "        return (1/(1+(scatter*Nmean)))\n",
    "    def r_negbin(scatter):\n",
    "        \"\"\"The r parameter of a negative binomial distribution\"\"\"\n",
    "        return 1/scatter\n",
    "    def negativebinomial(N, Nmean, scatter):\n",
    "        \"\"\"Returns the negative binomial distribution given a mean and intrinsic scatter\"\"\"\n",
    "        p = p_negbin(Nmean, scatter)\n",
    "        r = r_negbin(scatter)\n",
    "        return ((fGAMMA(N + r) / (fGAMMA(r)*fGAMMA(N+1)))*(p**r)*((1-p)**N))\n",
    "    \n",
    "    # Compute mu normalization, N_obs, and P(N_obs)\n",
    "    N = len(sigmas)\n",
    "    N_obs_error = munormerr\n",
    "    N_obs = munorm\n",
    "    Nmean = N_obs\n",
    "    P_Nobs = negativebinomial(N, Nmean, scatter_SI)\n",
    "    \n",
    "    # Compute mu_i values\n",
    "    mu_i = []\n",
    "    mu_i_error = []\n",
    "    for i in range(len(sigmas)):\n",
    "        mu_i_val = integratemu(sigmas[i],reffs[i],sigerr[i],refferr[i])\n",
    "        muimeanval = mu_i_val.mean\n",
    "        muierrorval = mu_i_val.sdev\n",
    "        mu_i.append(muimeanval)\n",
    "        mu_i_error.append(muierrorval)\n",
    "    mu_i = np.array(mu_i) / munorm\n",
    "    \n",
    "    # Compute neg2ln(L)\n",
    "    neg2lnL = -2*np.sum(np.log(mu_i)) - 2*np.log(P_Nobs) + ((sigma_CO-1)**2 / (0.19**2))\n",
    "    return neg2lnL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef0433",
   "metadata": {},
   "source": [
    "# Troubleshooting and Further Usage\n",
    "\n",
    "Note that this neg2lnL function keeps track of mu values and integration errors values, so simply editing the function to return these values can help with troubleshooting. With all of the galaxy data uncommented, this function takes a while to run for just one set of parameter values. Using multiple CPU cores at the same time can help with this, so I have included a script in the github repository to do this for the thermal relic mass values from 1 to 12. This can also be done with other parameter values, but the script would have to be modified. Note that this script also imports from a file called utils which is just the above function. I have also included in the repository the code that I used to compute the plot of -2lnL versus thermal relic mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1f0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
